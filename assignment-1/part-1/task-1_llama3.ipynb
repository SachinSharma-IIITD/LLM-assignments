{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type='nf8',  # Can be 'nf4' or 'fp4'\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16  # Adjust compute type if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09aa3534a85c4206a19610311ed12362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 14.96 GB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size_bytes = total_params * 2\n",
    "    total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "    print(f\"Model size: {total_size_gb:.2f} GB\")\n",
    "\n",
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_tokens=40):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a general purpose chatbot!\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    return f'Prompt: {prompt}\\n' + f'Response: {tokenizer.decode(response, skip_special_tokens=True)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: When did Albert Einstein and Stephen Hawking win the Nobel Prize?\n",
      "Response: Albert Einstein never won the Nobel Prize in Physics, despite being one of the most influential physicists of the 20th century. He was nominated for the prize several times, but the Nobel Committee did not\n"
     ]
    }
   ],
   "source": [
    "prompt = 'When did Albert Einstein and Stephen Hawking win the Nobel Prize?'\n",
    "output = generate_response(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Neil Armstrong did not eat on the Apollo 11 module. Yes or no?\n",
      "Response: Yes.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Neil Armstrong did not eat on the Apollo 11 module. Yes or no?'\n",
    "output = generate_response(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me about \"Aptomania\".\n",
      "Response: Aptomania! That's a fascinating topic. Aptomania refers to the widespread and intense enthusiasm for aptitude tests, particularly the popular ones like the Myers-Briggs Type Indicator (MB\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell me about \"Aptomania\".'\n",
    "output = generate_response(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 256789/5 = ? Just give the answer.\n",
      "Response: 51.36\n",
      "Prompt: If 256789 is divided by 5, what will be the result? Just give the answer.\n",
      "Response: 51.4\n",
      "Prompt: What is the result of 256789 divided by 5? Just give the answer.\n",
      "Response: 51375.8\n"
     ]
    }
   ],
   "source": [
    "prompt = '256789/5 = ? Just give the answer.'\n",
    "output1 = generate_response(prompt)\n",
    "print(output1)\n",
    "\n",
    "prompt = 'If 256789 is divided by 5, what will be the result? Just give the answer.'\n",
    "output2 = generate_response(prompt)\n",
    "print(output2)\n",
    "\n",
    "prompt = 'What is the result of 256789 divided by 5? Just give the answer.'\n",
    "output3 = generate_response(prompt)\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: If Ram has 14 apples and he gives 3 to Shyam and receives 4 from Ramesh, how many apples does Ram have now? Give number only.\n",
      "Response: 11\n",
      "Prompt: Out of a total of 20 apples 14 are given to Ram, and the remaining to Ramesh. Shyam receives 3 apples from Ram, who receives 4 from Ramesh. How many apples does Ram have now? Just give the number.\n",
      "Response: 8\n"
     ]
    }
   ],
   "source": [
    "prompt = 'If Ram has 14 apples and he gives 3 to Shyam and receives 4 from Ramesh, how many apples does Ram have now? Give number only.'\n",
    "output1 = generate_response(prompt)\n",
    "print(output1)\n",
    "\n",
    "prompt = 'Out of a total of 20 apples 14 are given to Ram, and the remaining to Ramesh. Shyam receives 3 apples from Ram, who receives 4 from Ramesh. How many apples does Ram have now? Just give the number.'\n",
    "output2 = generate_response(prompt)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who won the 2020 Mens Cricket World Cup?\n",
      "Response: The 2019 ICC Men's Cricket World Cup was won by the England national cricket team. They defeated New Zealand in the final at Lord's on July 14, 2019.\n",
      "Prompt: Which team won the Mens Cricket World Cup in 2020?\n",
      "Response: The ICC Men's Cricket World Cup 2019 was won by the England national cricket team, who defeated New Zealand in the final at Lord's on July 14, 2019. There was no Cricket World Cup held in 2020,\n",
      "Prompt: Did England win the 2020 Cricket World Cup? Yes or no?\n",
      "Response: Yes.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Who won the 2020 Mens Cricket World Cup?'\n",
    "output1 = generate_response(prompt)\n",
    "print(output1)\n",
    "\n",
    "prompt = 'Which team won the Mens Cricket World Cup in 2020?'\n",
    "output2 = generate_response(prompt, max_tokens=50)\n",
    "print(output2)\n",
    "\n",
    "prompt = 'Did England win the 2020 Cricket World Cup? Yes or no?'\n",
    "output3 = generate_response(prompt)\n",
    "print(output3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
